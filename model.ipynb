{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oz_esgWpEkS"
      },
      "source": [
        "1. Import the dataset into a Pandas DataFrame and do feature engineering.\n",
        "(a) Print the number of instances/samples in the DataFrame.\n",
        "(b) Print the total number of features in the DataFrame.\n",
        "(c) Print the total number of missing instances in the DataFrame.\n",
        "(d) Perform feature engineering to convert the Date/Time into other categorical features\n",
        "(e.g., year, month, day) and numerical value of time. See ‘import datetime’."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "HofNmUXZpEkV",
        "outputId": "3733a1ff-08d5-41d5-8164-1a03ed9c8a47"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('WindDataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvdP8vyEpEkX",
        "outputId": "624ed1d2-d796-481c-f456-e6f711737f8b"
      },
      "outputs": [],
      "source": [
        "print(\"1b. Number of Samples: \",len(df))\n",
        "print(\"1c. Number of Features: \", len(df.columns))\n",
        "print(\"1d. Number of Missing Instances: \", df.isna().sum().sum())\n",
        "\n",
        "# Dropped the missing values\n",
        "df = df.dropna()\n",
        "df = df[pd.to_numeric(df['MeasuredPower']) >= 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XAdQF6apEkY",
        "outputId": "3a31aa43-b4c4-4ac7-8ba3-9f09f67edacd"
      },
      "outputs": [],
      "source": [
        "print(\"1d. Converted DateTime into other categorical features:\")\n",
        "df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"])\n",
        "df['Year'] = df['DateTime'].dt.year\n",
        "df['Month'] = df['DateTime'].dt.month\n",
        "df['Day'] = df['DateTime'].dt.day\n",
        "df['Hour'] = df['DateTime'].dt.hour\n",
        "df['Minute'] = df['DateTime'].dt.minute\n",
        "df['Second'] = df['DateTime'].dt.second\n",
        "print(df.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "O6GJPaGxpEkZ",
        "outputId": "5e95b592-4056-4077-c1a0-ccda4db99aa7"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "print(\"2. Pair Plot of Data:\")\n",
        "print(\"The pairplot helps to visualize how feature 1 and feature 2 are related to one another. \\n For example Measured Power and Wind Speed shows a graph that at a certain wind speed, the measure power does not get any larger\")\n",
        "sns.pairplot(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "9j9JlZ7WpEka",
        "outputId": "90bb48bf-3b57-424e-cf90-88a0a0db2bba"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "corr = df.corr()\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "plt.title('Heatmap of Feature Correlations')\n",
        "plt.show()\n",
        "print(\"The heatmap shows linear correlations between 2 features. While month and measured power don't look correlated, they can still be since it only shows linearly correlated features. As expected, wind speed and measure power have a high correlation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "QA1Cem4ApEkb",
        "outputId": "b5a08fb5-d293-4544-9e15-4c96f60772e4"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(x='Month', y='MeasuredPower', data=df)\n",
        "plt.title(\"Boxplot of Monthly Power Distribution\")\n",
        "plt.show()\n",
        "print(\"This boxplot shows the non linear correlation between the month and measured power. From the graph you can see that the first half, the measure power goes down, and then around July it starts increasing again. This could happen for many reasons such as change in wind patterns or temperature.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUEC5D_lpEkc"
      },
      "source": [
        "3. Is there any difference between the months for average power production? Which month has\n",
        "the highest average power production, and which has the lowest? (Hint: Pandas can group by\n",
        "columns and report means. See pandas.DataFrame.group_by or pandas.DataFrame.pivot_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvOZz9jRpEke",
        "outputId": "d9de5f8a-92d9-4422-b9ee-8b0d550611a4"
      },
      "outputs": [],
      "source": [
        "print(\"Based on the boxplot from question 2, you can see that there is a difference between the month and its average power production.\")\n",
        "print(\"It is very close but March has the highest average power production while \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4aeBGympEkf",
        "outputId": "0cca31f1-3b8f-4bc9-bed6-51fe91a90310"
      },
      "outputs": [],
      "source": [
        "print(\"4: Features Pearson Correlation\")\n",
        "corr = df.corr(method='pearson')\n",
        "print(corr)\n",
        "print(\"Since Pearson Correlation measures the linear relation between the two features in the grid,\")\n",
        "print(\"we can see that features like MeasuredPower, WindSpeed, and TheoreticalPower are positively linearly\")\n",
        "print(\"correlated, as can be seen by their Pearson Correlation values being close to one.\")\n",
        "print(\"All other features aren't very linearly correlated with any of the others, as the magnitude of their correlation values are less than 0.1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ha9O3kGUpEkf",
        "outputId": "d779169d-b9f2-4826-eaf9-d43333c49ace"
      },
      "outputs": [],
      "source": [
        "print(\"5: Maximum Wind Speed\")\n",
        "max_wind_speed = df['WindSpeed'].max()\n",
        "max_wind_speed_direction = df.loc[df['WindSpeed'] == max_wind_speed, 'WindDirection'].values[0]\n",
        "print(\"The maximum wind speed is\",max_wind_speed)\n",
        "print(\"The direction of the wind that resulted in the maximum wind speed is\",max_wind_speed_direction,\"degrees\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlEu6WWnpEkg",
        "outputId": "8adba8b6-a571-4dde-bcbf-c5922664fb8a"
      },
      "outputs": [],
      "source": [
        "print(\"6: Minimum Wind Speed\")\n",
        "min_wind_speed = df['WindSpeed'].min()\n",
        "min_wind_speed_year = df.loc[df['WindSpeed'] == min_wind_speed, 'Year'].values[0]\n",
        "min_wind_speed_month = df.loc[df['WindSpeed'] == min_wind_speed, 'Month'].values[0]\n",
        "min_wind_speed_day = df.loc[df['WindSpeed'] == min_wind_speed, 'Day'].values[0]\n",
        "min_wind_speed_hour = df.loc[df['WindSpeed'] == min_wind_speed, 'Hour'].values[0]\n",
        "min_wind_speed_minute = df.loc[df['WindSpeed'] == min_wind_speed, 'Minute'].values[0]\n",
        "\n",
        "print(\"The minimum wind speed is\",min_wind_speed)\n",
        "print(\"The minimum wind speed occurred at {}:{} on {}-{}-{}\".format(min_wind_speed_hour,min_wind_speed_minute,min_wind_speed_day,min_wind_speed_month,min_wind_speed_year))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz7_H41jpEkh"
      },
      "source": [
        "7. Does the manufacturer’s theoretical power production curve fit well with the real power\n",
        "production?\n",
        "(a) Plot the theoretical power (TheoreticalPower) and real power (MeasuredPower) vs.\n",
        "windspeed\n",
        "(b) Based on (a) it should be evident that there are outliers in the real production of\n",
        "power. Remove the outlier data at wind speed > 2.5 m/s and zero power production. (Otherwise,\n",
        "it will be hard to train an accurate model in Part 8)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "gN3BtLmupEki",
        "outputId": "1214b245-6454-4eb9-ba89-b1244b2f4af9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a scatter plot of measured power vs. windspeed\n",
        "plt.scatter(df[\"WindSpeed\"], df[\"MeasuredPower\"], label='Measured Power')\n",
        "\n",
        "plt.scatter(df[\"WindSpeed\"], df[\"TheoreticalPower\"], label='Theoretical Power')\n",
        "\n",
        "\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel(\"Windspeed (m/s)\")\n",
        "plt.ylabel(\"Theoretical/Measured Power (kW)\")\n",
        "plt.title(\"Theoretical/Measured Power vs. Windspeed\")\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "W6j2dm3ppEkj",
        "outputId": "1e15a59a-6707-40e8-99e2-b409ba6ba8a1"
      },
      "outputs": [],
      "source": [
        "df = df.loc[(df[\"WindSpeed\"] <= 2.5) | (df[\"MeasuredPower\"] != 0)]\n",
        "# Create a scatter plot of measured power vs. windspeed\n",
        "plt.scatter(df[\"WindSpeed\"], df[\"MeasuredPower\"], label='Measured Power')\n",
        "\n",
        "plt.scatter(df[\"WindSpeed\"], df[\"TheoreticalPower\"], label='Theoretical Power')\n",
        "\n",
        "\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel(\"Windspeed (m/s)\")\n",
        "plt.ylabel(\"Theoretical/Measured Power (kW)\")\n",
        "plt.title(\"Theoretical/Measured Power vs. Windspeed\")\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y23ePIufpEkj"
      },
      "outputs": [],
      "source": [
        "df = df.drop([\"Year\", \"Day\"], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTigW3ILpEkk"
      },
      "source": [
        "8. Train and test two different machine learning models to predict real power (MeasuredPower)\n",
        "using good data science practices. Note: don’t use TheoreticalPower as a feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qyJ5SanpEkk"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score  \n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "X = df.drop([\"TheoreticalPower\", \"MeasuredPower\", \"DateTime\"], axis=1)\n",
        "y = df[\"MeasuredPower\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkhbLNiHpEkk"
      },
      "source": [
        "Random Forest Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsoNyKO_pEkl"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "XtrainRFR, XtestRFR, ytrainRFR, ytestRFR = train_test_split(X, y, random_state=27, train_size=0.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1KnwMZgpEkl"
      },
      "source": [
        "(a) Justify your train/test split ratio and your ML model selection choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7CQHW_HpEkl",
        "outputId": "bae8e2e5-b5d8-4434-e645-24381bf7d267"
      },
      "outputs": [],
      "source": [
        "print(\"With the size of the data being over 50,000 data points, we believe a 70/30 train/test split should be sufficient\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQwyRxNRpEkm"
      },
      "source": [
        "(b) Use GridSearchCV to optimize the model hyperparameters. Justify your choices of\n",
        "which hyperparameters to optimize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNju4-auVOES",
        "outputId": "aea12b05-9ddb-477b-a459-bc45915e0a4e"
      },
      "outputs": [],
      "source": [
        "params = {'n_estimators': [100, 150, 200, 250, 300, 350], 'max_depth': [17, 19, 21], 'min_samples_split': [2, 3, 5], \"criterion\": [\"squared_error\", \"poisson\"]}\n",
        "#for criterion: absolute error takes too long and poisson doesn't allow negative values(may need to remove them)\n",
        "\n",
        "\n",
        "rfr = GridSearchCV(RandomForestRegressor(random_state=27), params, n_jobs = -1, verbose = 4, cv=5)\n",
        "rfr.fit(XtrainRFR, ytrainRFR)\n",
        "print(\"Best hyperparameters: \", rfr.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPC7fweStQz2"
      },
      "source": [
        "[testing lower values:\n",
        "\n",
        "abs-error : 6m 50sec\n",
        "\n",
        "sq-error : 5sec\n",
        "\n",
        "friedman : 5sec\n",
        "\n",
        "poisson : has negative values- cannot run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcevcfH1pEkn"
      },
      "source": [
        "(c) Report the best_estimator_, best_score_ and best_params_ identified from GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FHeqkzL2pEkn",
        "outputId": "a99a370f-efa5-4d24-84d0-4bd32155034c"
      },
      "outputs": [],
      "source": [
        "y_predRFR = rfr.predict(XtestRFR)\n",
        "print(\"The best estimator is:\",rfr.best_estimator_)\n",
        "print(\"The best score is:\",rfr.best_score_)\n",
        "print(\"The best parameters is:\",rfr.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo-x1SQmpEkn"
      },
      "source": [
        "(d) What is the Mean Absolute Error (MAE) and root-mean-square error (RMSE) on the\n",
        "training and test sets for each model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HywUGVSHpEko"
      },
      "outputs": [],
      "source": [
        "\n",
        "ypredTrainRFR = rfr.predict(XtrainRFR)\n",
        "mae_train = mean_absolute_error(ytrainRFR, ypredTrainRFR)\n",
        "rmse_train = mean_squared_error(ytrainRFR, ypredTrainRFR, squared=False)\n",
        "\n",
        "# Compute MAE and RMSE on the test set\n",
        "mae_test = mean_absolute_error(ytestRFR, y_predRFR)\n",
        "rmse_test = mean_squared_error(ytestRFR, y_predRFR, squared=False)\n",
        "\n",
        "# Print the results\n",
        "print(\"Training set MAE:\", mae_train)\n",
        "print(\"Training set RMSE:\", rmse_train)\n",
        "print(\"Test set MAE:\", mae_test)\n",
        "print(\"Test set RMSE:\", rmse_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1A0QCmTpEko"
      },
      "source": [
        "Gradient Boosting Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdS8UmBApEko"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "XtrainGBR, XtestGBR, ytrainGBR, ytestGBR = train_test_split(X, y, random_state=27, train_size=0.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwB5yz5opEko"
      },
      "source": [
        "(a) Justify your train/test split ratio and your ML model selection choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VloI-eJqpEkp"
      },
      "source": [
        "We chose the train/test split because it is pretty standard to use 70/30. This ratio ensures there is enough training data to actually train the model and enough test data to actually learn how well the model did."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfDDncajpEkp"
      },
      "source": [
        "(b) Use GridSearchCV to optimize the model hyperparameters. Justify your choices of\n",
        "which hyperparameters to optimize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "RxAVRlaZpEkp",
        "outputId": "4885693c-52cc-4f5a-a933-0a5afd803c95"
      },
      "outputs": [],
      "source": [
        "parameters = {'learning_rate': [0.01, 0.05, 0.1], 'n_estimators': [100, 150, 250], 'subsample': [0.8, 0.3, 0.1], 'max_depth': [3, 4, 8]}\n",
        "\n",
        "regressor = GradientBoostingRegressor()\n",
        "\n",
        "gbr = GridSearchCV(regressor, param_grid = parameters, cv = 5, verbose = 4, n_jobs=-1)\n",
        "gbr.fit(XtrainGBR, ytrainGBR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXxcZ3PvpEkp"
      },
      "source": [
        "(c) Report the best_estimator_, best_score_ and best_params_ identified from GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWdF_cIwpEkp",
        "outputId": "42e4c66d-5944-49db-a950-cf709ac3cb76"
      },
      "outputs": [],
      "source": [
        "y_predGBR = gbr.predict(XtestGBR)\n",
        "print(\"The best estimator is:\",gbr.best_estimator_)\n",
        "print(\"The best score is:\",gbr.best_score_)\n",
        "print(\"The best parameters is:\",gbr.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9soc5gKpEkq"
      },
      "source": [
        "(d) What is the Mean Absolute Error (MAE) and root-mean-square error (RMSE) on the\n",
        "training and test sets for each model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULmeBjm1pEkq"
      },
      "outputs": [],
      "source": [
        "ypredTrainGBR = gbr.predict(XtrainGBR)\n",
        "mae_trainGBR = mean_absolute_error(ytrainGBR, ypredTrainGBR)\n",
        "rmse_trainGBR = mean_squared_error(ytrainGBR, ypredTrainGBR, squared=False)\n",
        "\n",
        "# Compute MAE and RMSE on the test set\n",
        "mae_testGBR = mean_absolute_error(ytestGBR, y_predGBR)\n",
        "rmse_testGBR = mean_squared_error(ytestGBR, y_predGBR, squared=False)\n",
        "\n",
        "# Print the results\n",
        "print(\"Training set MAE:\", mae_trainGBR)\n",
        "print(\"Training set RMSE:\", rmse_trainGBR)\n",
        "print(\"Test set MAE:\", mae_testGBR)\n",
        "print(\"Test set RMSE:\", rmse_testGBR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "residual_train = ytrainGBR - ypredTrainGBR\n",
        "residual_test = ytestGBR - y_predGBR\n",
        "\n",
        "plt.scatter(ypredTrainGBR, residual_train, label='Training data')\n",
        "plt.scatter(y_predGBR, residual_test, label='Test data')\n",
        "plt.axhline(y=0, color='r', linestyle='-')\n",
        "plt.title('Residual Error for Train and Test Sets')\n",
        "plt.xlabel('Predicted values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.scatter(ytrainGBR, ypredTrainGBR, label='Training data')\n",
        "plt.scatter(ytestGBR, y_predGBR, label='Test data')\n",
        "plt.xlabel('Actual values')\n",
        "plt.ylabel('Predicted values')\n",
        "plt.title('R^2 of Model Prediction vs. Actual Values')\n",
        "plt.plot([ytrainGBR.min(), ytrainGBR.max()], [ytrainGBR.min(), ytrainGBR.max()], color='r')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "10. Creating our 3rd model using K Nearest Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Assuming X is the matrix of input features and y is the vector of target values\n",
        "X_trainKNN, X_testKNN, y_trainKNN, y_testKNN = train_test_split(X, y, test_size=0.3, random_state=27)\n",
        "\n",
        "\n",
        "knn = KNeighborsRegressor(n_neighbors=5)\n",
        "\n",
        "\n",
        "knn.fit(X_trainKNN, y_trainKNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_predTEST = knn.predict(X_testKNN)\n",
        "ypredTrainKNN = knn.predict(X_trainKNN)\n",
        "mae_trainKNN = mean_absolute_error(y_trainKNN, ypredTrainKNN)\n",
        "rmse_trainKNN = mean_squared_error(y_trainKNN, ypredTrainKNN, squared=False)\n",
        "\n",
        "# Compute MAE and RMSE on the test set\n",
        "mae_testKNN = mean_absolute_error(y_testKNN, y_predTEST)\n",
        "rmse_testKNN = mean_squared_error(y_testKNN, y_predTEST, squared=False)\n",
        "\n",
        "print(\"Training set MAE:\", mae_trainKNN)\n",
        "print(\"Training set RMSE:\", rmse_trainKNN)\n",
        "print(\"Test set MAE:\", mae_testKNN)\n",
        "print(\"Test set RMSE:\", rmse_testKNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the data\n",
        "train_mae = [mae_trainKNN, mae_train, mae_trainGBR] # Train MAE values for each model\n",
        "test_mae = [mae_testKNN, mae_test, mae_testGBR] # Test MAE values for each model\n",
        "models = ['KNN', 'RFR', 'GBR'] # Labels for each model\n",
        "\n",
        "# Set the position of the bars on the x-axis\n",
        "bar_width = 0.4\n",
        "r1 = np.arange(len(train_mae))\n",
        "r2 = [x + bar_width for x in r1]\n",
        "\n",
        "# Create the bar chart\n",
        "plt.bar(r1, train_mae, color='blue', width=bar_width, edgecolor='white', label='Train MAE')\n",
        "plt.bar(r2, test_mae, color='green', width=bar_width, edgecolor='white', label='Test MAE')\n",
        "\n",
        "# Add x-axis labels and a title\n",
        "plt.xlabel('Models')\n",
        "plt.xticks([r + bar_width/2 for r in range(len(train_mae))], models)\n",
        "plt.ylabel('MAE')\n",
        "plt.title('Comparison of MAE for Three Regression Models')\n",
        "\n",
        "# Add a legend\n",
        "plt.legend()\n",
        "\n",
        "# Show the chart\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "9. (a) Which of your implemented regressor models performed the best? With which feature\n",
        "space? According to which performance metrics?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The best regressor model that we chose was Gradient Boosting Regressor. The features that we used were WindSpeed, WindDirection, Month, Hour, Minute, and Second. While minute and second aren't the most useful, we did see some difference in MAE and RMSE values. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(b) What agrees better with the measured power—the theoretical power or the predicted power\n",
        "via your best trained ML model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (45182,) (13555,) ",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\brian\\Desktop\\freshsem2\\engr100\\Project\\model.ipynb Cell 49\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brian/Desktop/freshsem2/engr100/Project/model.ipynb#Y204sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m theoretical_power_error \u001b[39m=\u001b[39m \u001b[39mabs\u001b[39m(y \u001b[39m-\u001b[39m theoretical_power)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brian/Desktop/freshsem2/engr100/Project/model.ipynb#Y204sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Compute the error between the measured power and the predicted power via the best trained ML model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/brian/Desktop/freshsem2/engr100/Project/model.ipynb#Y204sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m predicted_power_error \u001b[39m=\u001b[39m \u001b[39mabs\u001b[39m(y \u001b[39m-\u001b[39;49m y_predGBR)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brian/Desktop/freshsem2/engr100/Project/model.ipynb#Y204sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Compare the errors and determine which one agrees better with the measured power\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brian/Desktop/freshsem2/engr100/Project/model.ipynb#Y204sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mmean(predicted_power_error) \u001b[39m<\u001b[39m np\u001b[39m.\u001b[39mmean(theoretical_power_error):\n",
            "File \u001b[1;32mc:\\Users\\brian\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\ops\\common.py:70\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[0;32m     68\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 70\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
            "File \u001b[1;32mc:\\Users\\brian\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\arraylike.py:108\u001b[0m, in \u001b[0;36mOpsMixin.__sub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__sub__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    107\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__sub__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[1;32m--> 108\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_arith_method(other, operator\u001b[39m.\u001b[39;49msub)\n",
            "File \u001b[1;32mc:\\Users\\brian\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\series.py:5639\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   5637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_arith_method\u001b[39m(\u001b[39mself\u001b[39m, other, op):\n\u001b[0;32m   5638\u001b[0m     \u001b[39mself\u001b[39m, other \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39malign_method_SERIES(\u001b[39mself\u001b[39m, other)\n\u001b[1;32m-> 5639\u001b[0m     \u001b[39mreturn\u001b[39;00m base\u001b[39m.\u001b[39;49mIndexOpsMixin\u001b[39m.\u001b[39;49m_arith_method(\u001b[39mself\u001b[39;49m, other, op)\n",
            "File \u001b[1;32mc:\\Users\\brian\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\base.py:1295\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   1292\u001b[0m rvalues \u001b[39m=\u001b[39m ensure_wrapped_if_datetimelike(rvalues)\n\u001b[0;32m   1294\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1295\u001b[0m     result \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49marithmetic_op(lvalues, rvalues, op)\n\u001b[0;32m   1297\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_result(result, name\u001b[39m=\u001b[39mres_name)\n",
            "File \u001b[1;32mc:\\Users\\brian\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:222\u001b[0m, in \u001b[0;36marithmetic_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     \u001b[39m# TODO we should handle EAs consistently and move this check before the if/else\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[39m# (https://github.com/pandas-dev/pandas/issues/41165)\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     _bool_arith_check(op, left, right)\n\u001b[1;32m--> 222\u001b[0m     res_values \u001b[39m=\u001b[39m _na_arithmetic_op(left, right, op)\n\u001b[0;32m    224\u001b[0m \u001b[39mreturn\u001b[39;00m res_values\n",
            "File \u001b[1;32mc:\\Users\\brian\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:163\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    160\u001b[0m     func \u001b[39m=\u001b[39m partial(expressions\u001b[39m.\u001b[39mevaluate, op)\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     result \u001b[39m=\u001b[39m func(left, right)\n\u001b[0;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_cmp \u001b[39mand\u001b[39;00m (is_object_dtype(left\u001b[39m.\u001b[39mdtype) \u001b[39mor\u001b[39;00m is_object_dtype(right)):\n\u001b[0;32m    166\u001b[0m         \u001b[39m# For object dtype, fallback to a masked operation (only operating\u001b[39;00m\n\u001b[0;32m    167\u001b[0m         \u001b[39m#  on the non-missing values)\u001b[39;00m\n\u001b[0;32m    168\u001b[0m         \u001b[39m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[0;32m    169\u001b[0m         \u001b[39m#  incorrectly, see GH#32047\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\brian\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:239\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39mif\u001b[39;00m op_str \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    237\u001b[0m     \u001b[39mif\u001b[39;00m use_numexpr:\n\u001b[0;32m    238\u001b[0m         \u001b[39m# error: \"None\" not callable\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m         \u001b[39mreturn\u001b[39;00m _evaluate(op, op_str, a, b)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[39mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
            "File \u001b[1;32mc:\\Users\\brian\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:69\u001b[0m, in \u001b[0;36m_evaluate_standard\u001b[1;34m(op, op_str, a, b)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mif\u001b[39;00m _TEST_MODE:\n\u001b[0;32m     68\u001b[0m     _store_test_result(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m op(a, b)\n",
            "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (45182,) (13555,) "
          ]
        }
      ],
      "source": [
        "theoretical_power = df[\"TheoreticalPower\"]\n",
        "theoretical_power_error = abs(y - theoretical_power)\n",
        "\n",
        "# Compute the error between the measured power and the predicted power via the best trained ML model\n",
        "predicted_power_error = abs(y - y_predGBR)\n",
        "\n",
        "# Compare the errors and determine which one agrees better with the measured power\n",
        "if np.mean(predicted_power_error) < np.mean(theoretical_power_error):\n",
        "    print(\"The predicted power via the best trained ML model agrees better with the measured power.\")\n",
        "else:\n",
        "    print(\"The theoretical power agrees better with the measured power.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
